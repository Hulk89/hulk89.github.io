---
layout: post
title:  "NMT & seq2seq Models : A Tutorial chapter 7 - Generating output"
date:   2017-04-14
category: "neural machine translation"
tags: [seq2seq, beam search, length bias]
---


## Generating Output

* Random sampling
  * $P(E \vert  F)$에서 $E$를 random하게 뽑는다.
  * 특정 input에 대해서 다양한 output을 얻길 원할 때 유용하다.
    * 예> 메신저 봇

* 1-best Search (greedy)
  * $\hat{E}=\underset{E}{argmax}P(E \vert  F)$
  * 매 step마다 argmax를 뽑고 그 다음 step의 condition으로 넣는 방법.
    * 단점 : 가장 높은 probability를 갖는 sequence를 내지 않을 때가 많다.
* n-best Search (beam search)
  * n개의 best hypothesis를 유지한다.
  * beam size가 커질수록, 심각한 length bias가 나타난다.
  * 이를 줄이기 위해 다양한 방법들이 존재.
    * sentence의 length에 대한 prior probability를 넣는다.
      * $\hat{E} = \underset{E}{argmax}\ logP(\vert E\vert \ \vert \ \vert F\vert ) + logP(E\ \vert \ F)$
        * where $$P(\vert E\vert \ \vert \ \vert F\vert )=\frac{c(\vert E\vert \ \vert \ \vert F\vert )}{c(\vert F\vert )}$$
    * 그냥 output length로 나눠버리는 방법
      * $\hat{E} = \underset{E}{argmax}\ logP(E\ \vert \ F) /\vert E\vert $

### length bias
$P(E\vert F) = \Pi_t^{\vert E\vert }P(e_t\vert F,e_1^{t-1})$ 처럼 Markov chain을 가정한 model들은 짧은 문장의 확률이 훨씬 높다.
> 단어가 많아질 수록, 해당 단어의 확률을 곱해야하는데 그럴수록 전체 문장의 확률은 작아진다.


