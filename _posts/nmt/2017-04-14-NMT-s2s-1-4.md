---
layout: post
title:  "Neural Machine Translation & sequence-to-sequence Models : A Tutorial chapter 1-4"
date:   2017-04-14
category: "neural machine translation"
tags: [seq2seq, RNN, smt, nmt, n-gram]
---


# 단어 정의

* source language : machine translation system의 input
* target language : machine translation system의 output
* parallel copora : 두 언어의 문장이 쌍으로 묶여있는 데이터 집합
* unknown words: training set에는 없는데, test set에는 나오는 단어들
---

# 1. machine translation
machine translation을 다음처럼 정의한다.
* source sentence : $F = f_1,...,f_J = f_1^{\vert F\vert }$ 를 받아서
* target sentence : $E = e_1,...,e_I = e_1^{\vert E\vert }$ 로 변환하는 시스템

즉 함수로는 다음처럼 쓸 수 있다.
$$ \hat{E}=mt(F) $$
여기서 $\hat{E}$는 input으로 F가 주어졌을 때의 translation hypothesis이디.

---

# 2. Statistical Machine Translation(SMT)

* F가 주어졌을 때, E의 확률모델을 세워서 번역을 한다.
  * $P(\ E\ \vert \ F; \theta\ )$ 
* target sentence를 찾는 방법: $\hat E = \underset{E}{argmax}\ P(\ E\ \vert \ F;\theta\ )$
  * $\theta$ : probability distribution을 특정하는 파라미터
  * $\theta$는 parallel copora로 트레이닝시킨다.

## 필요한 요소들
* Modeling
  * $P\ (\ E\ \vert \ F;\theta\ )$를 어떻게 만들지 정해야한다. 
    * ex > 어떤 parameter를 가질지
    * ex > 어떻게 parameter들이 prob. didst.를 특정하게 할지..
* Learning
  * training data를 사용해서 어떻게 parameter $\theta$를 학습시키는 방법이 필요하다.
* Search
  * best hypothesis를 찾아 가장 그럴싸한 sentence를 모델에서 뽑아내는 작업.
  * decoding이라고도 부른다.

SMT의 목적은 source sentence가 주어질 때, target sentence를 만드는 모델에 관한 내용이다. 이를 더 얘기하기 전에, language model에 관해 알아보자.

---

# 3. n-gram Language Models

Language Model : $P(E)$ 
* target sentence $E$에만 관련됨. source sentence는 주어지지않는다. 이런걸 왜 만들까?
  * 자연스러운 문장인지 판단 가능 : target language로 된 어떤 문장이 왔을 때, language model로 얼마나 자연스러운 문장인지 판단 가능.
  * text 생성 가능 : P(E)에서 sampling을 계속 하면 text 생성이 가능하다.

## Word-by-word Computation of probabilities

$P(E)=P(\vert E\vert =T,e_1^T)$를 구하고 싶은 것. 

* naive : 각 time-step을 independent한 변수로 보고 joint probability를 구해야할 것이다. 
* 자주 쓰는 모델 : $P(E)=\Pi_t^{T+1}{P(e_t\ \vert \ e_1^{t-1})}$ ----(1)
    > $e_{T+1}$ : *sentence end symbol* `</s>`

(1)을 세우면 language model을 만드는 것은 $P(e_t\vert e_1^{T-1})$를 계산하는 것이 된다. 이는 naive보다 훨씬 쉽다.

이제 Language model중 n-gram LM을 살펴보자!

## Count-based n-gram Language Models

$c_{prefix}(A)$ : 전체 training set에서 `문장 처음부터` $A$라는 string이 얼마나 나왔는지 세주는 함수.

$$P_{ML}(e_t\vert e_1^{t-1})=\frac{c_{prefix}(e_1^t)}{c_{prefix}(e_1^{t-1})}\ \ \ \ \ \ \ \ \ -(2)$$<br>

(2)는 문장의 맨 처음부터 substring이 나왔을 때, 그 다음이 $e_t$인 경우를 구한다. 

* 문제점 : 아주 비슷한 sentence라도 없는거면 확률이 0. + 긴 문장일 수록 확률이 0이 될 확률이 높아짐
  * 해결법 : 고정된 window를 두어서 확률을 높이자! => `n-gram`
  
### n-gram model
어떤 글자의 확률을 구할 때, 앞의 `n-1`개의 단어로 유추하는 방법.

$$P(e_t\vert e_1^{t-1}) ~= P_{ML}(e_t\vert e_{t-n+1}^{t-1})$$

* parameter $\theta$
  * $$\theta_{e^t_{t-n+1}}=P_{ML}(e_t\vert e_{t-n+1}^{t-1})=\frac{c(e_{t-n+1}^{t})}{c(e_{t-n+1}^{t-1})}$$
  
### smoothing
문제 : 윈도우를 적게 두어도, window size안에 겹치는 안본 단어는 확률이 0이다.
해결 : 여러 개의 n-gram을 섞어서 해결(interpolation) 
$$P(e_t\vert e_{t-1}) = (1-a)P_{ML}(e_t\vert e_{t-1}) + aP_{ML}(e_t)$$
> bigram의 interpolation 수식...

smoothing 관련 여러 방식들이 있음
* context-dependent smoothing coefficients
  * context에 따라 $a$를 다른 값을 줌. 휴리스틱 또는 data를 통해 learning한다.
* Back-off
  * n-gram의 확률이 0인것만 (n-1)-gram에서 끌어옴.
  * interpolation과 비슷한 결과를 낸다고..
* Modified distribution
  * MLE대신에 다른 distribution을 쓸 수 있다.
  * ex > discounting : count시에 상수값을 빼줌.

요새는 MKN(Modified Kneser-Ney smoothing)이 젤 잘나간대...

## LM Evaluation

LM을 만들고나면, 제대로 동작하는지 알아야하지..
먼저 데이터를 3개의 set으로 나눈다.
* Training data : parameters $\theta$를 training하기 위한 data set
* Development data (validation data): hyperparam을 조정하거나, 모델구조를 선택하는데 쓰임
* Test data : 선택된 모델의 성능 측정을 위해 씀

$\xi_{test}$를 test set이라 하면 likelihood를 다음처럼 쓸 수 있다.

$$P(\xi_{test}\ \vert \ \theta) = \Pi_{E \in \xi_{test}}P(E\ ;\ \theta) $$

근데 사실 log-likelihood를 더 많이 씀
$$log P(\xi_{test}\ \vert \ \theta) = \sum_{E \in \xi_{test}}P(E\ ;\ \theta) -----(3)$$
* P가 이미 작은데 곱하면 numerical error가 생김.
* 미분시 편하다.

근데 (3)에 testset의 전체 단어 갯수로 나눈걸 많이 씀.
$$length(\xi_{test})=\sum_{E\in \xi_{test}}\vert E\vert $$

### Perplexity
>요 부분은 혼자 열심히 삽질해본 [perplexity]({% post_url 2017-04-14-perplexity %}) 항목을 먼저 보면 좋다.

모르는 어떤 distributiuon p를 모델링한 dist q가 존재할 때, q의 perplexity는
$$b^{-\frac{1}{N}\sum_i^Nlog_bq(x_i)}$$
로 정의된다. 
여기서 $\hat{p}(x)$를 $x_i$의 빈도수라 생각하면, $b$의 지수는 cross-entropy로 볼 수 있다. 그렇다면 q가 잘 모델링 안되었을 때, perplexity가 커지겠다.

어쨌든 전 챕터에서 정의한대로 perplexity를 만들어보면
$$ppl(\xi_{test}\ ;\ \theta)= e^{-(logP(\xi_{test}\ ; \theta))/length(\xi_{test})}$$
가 된다.
perplexity가 커질수록, 사람이 봤을때 더 이상한 것을 잘 느낄 수 있다고 한다.

## unknown word 다루기.
* 없는 단어에 대해 확률 0을 주기
* unknown word distribution을 $p_{unk}(e_t)$를 만들어 interpolation하기
* `<unk>`를 단어장에 추가하기
  * train set에 잘 안나오는 녀석을 `<unk>`로 치환
  * $P_{unk}(e_t)$에서 실제 word 뽑기

---

# 4. Log-linear Language Models

n-gram LM과 비슷하게 `context` $e_{t-n+1}^{t-1}$이 주어질 때 `특정 단어` $e_t$의 확률을 구하는 모델이다. 그러나 이 방법은 count-based LM과는 많이 다르며, 다음 절차를 따른다.

* feature 계산 : `feature function`  $\Phi(e_{t-n+1}^{t-1}) \mapsto x$는 context를 $x \in \mathbb{R}^N$로 보내는 함수이다. 이 때, $x$를 `feature vector`라 한다.
  * ex > one-hot vector는 feature vector의 한 종류이다.
  * 여러 개의 word를 보려면 concatenation 시킨다는데... 다른 방법도 많겠지.
* score 계산 : $W \in \mathbb{R}^{\vert V\vert  \times N},\ b \in \mathbb{R}^{\vert V\vert }$ 를 가지고 feature를 vocab에 매핑시켜버린다.
  * $s = Wx + b$
  * 실제론 계산량을 줄이기 위해 이런 방법을 쓴다.
    * $s = \sum_{\{j : x_j \neq 0\}} W_{:,j}x_j + b$ 
* probability 계산: score에 `softmax`를 사용해서 각 단어가 나올 확률을 계산한다.
  * $p = softmax(s)$
  
## Learning Model Parameters

* parameter $\theta$ : $W,\ b$
  * `loss function`을 정의
    * 얼마나 모델이 안좋은지 측정하는 함수
    * `negative log likelihood`를 쓴다.
$$l(\xi_{train},\ \theta)=-logP(\xi_{train}\ \vert \ \theta)=-\sum_{E \in \xi_{train}}logP(E\ \vert \ \theta)$$
  * SGD로 loss를 줄이는 방향으로 training
    * 하나의 단어 또는 batch를 random하게 pick하고 
    * likelihood가 향상되는 방향으로 step
$$\theta \leftarrow \theta - \eta\frac{dl(e^t_{t-n+1})}{d\theta}$$
    * $\eta$: learning rate
  * 유의할 사항
    * $\eta$를 어떻게 설정하는가
      * 너무 작으면 : local minima에 잘빠짐 / 학습이 너무 느림
      * 너무 크면 : 발산하거나 / unstable
      * 해결법 : `learning decay`
        * 처음에는 크게 주고 training 시간이 지날수록 $\eta$를 줄임
    * Early stopping : validation set으로 모델성능을 측정했을 때, 가장 좋은 녀석을 저장함
      * model이 overfit되기 시작할 때 좋다.
    * training batch를 shuffle
* SGD 변종들
  * `momentumSGD` : 관성을 주는 방법
  * `AdaGrad` : 많이 업데이트 되는 녀석과 적게 업데이트되는 녀석이 다른 lr-decay를 가지도록..
  * `Adam` : adagrad와 비슷한데 더 빠른 convergence를 가짐. overfit에 더 잘빠진다고 함.

~~미분하는 부분은 자체검열~~

## Other Features for LM

log-linear model은 예측에 좋다고 생각하는 것들을 디자인할 수 있도록 유연성을 가진다.

* `context word features` : 위에서 실컷 다룬거...
* `context class` : context word들중 비슷한 것끼리 묶은 것. 모델이 더 일반화가 잘된다.
* `context suffix features` : ing같이 계속 반복되는 녀석들은 없애버리는 feature. 얘도 일반화 OK
* `Bag-of-words features` : 이전 n개 단어를 쓰는것 대신, 문장의 모든 이전단어들을 쓸 수 있다. 근데 concatenation하지말고, 더해버림. 위치 정보를 잃는 대신, 어떤 단어들이 주로 많이 쓰였냐를 볼 수 있다.

# LM에 대한 생각

n-gram LM은 가장 단순한 LM이다. 이 것은 parameter를 training set에서 직접구할 수 있으며, variation이 상대적으로 적다. 

log-linear model로 넘어가면서, feature를 넣어 variation도 많아지게 되었다. 그러나 parameter를 직접구할 수 없게되었고, optimization process가 추가되었다. 