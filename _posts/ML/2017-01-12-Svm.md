---
layout: post
title:  "02. svm 구현"
date:   2017-01-12
category: "neural network"
tags: [ml, nn, svm]
---

[01. softmax classifier와 neural network 구현]({% post_url 2017-01-12-SoftMax-NN %})

[03. RNN 구현]({% post_url 2017-01-13-RNN %})

저번시간에 공부한 것과 별로 달라지는 것이 없다. softmax를 먼저 보고 봐야함. 

# Svm

## loss function

전체 loss function은 아예 똑같지..
$$L = \frac{1}{N} \sum_i L_i + \frac{1}{2}\lambda \sum_k \sum_l W^2_{k,l}$$
$L_i$가 다음과 같이
$$L_i = \sum_{j \neq y_i} max(0,\ f_j - f_{y_i} + \Delta)$$
로 정의되며 $\frac{\partial L_i}{\partial f_k}$를 구하면 비슷해지겠지?

## $\frac{\partial L_i}{\partial f_k}$ 구해보자!

1. $k \neq y_i$이고 $L_{i,k}$가 0보다 클 때 =>  $\frac{\partial L_i}{\partial f_k} = 1$
2. 나머지 경우 => $\frac{\partial L_i}{\partial f_k} = 0$

이는 forward pass시에 구할 수 있다. 코드만 살펴보면 되겠다.

```python
import matplotlib.pyplot as plt
import numpy as np
from data import getData, showData

X, y, K = getData()
N, D = X.shape

# hyperparameters
step_size = 1e-0
reg = 1e-3
delta = 1
# initialize parameters randomly
W = 0.01 * np.random.randn(D, K)
b = np.zeros((1, K))

for i in range(200):
    scores = np.dot(X, W) + b
    # forward pass
    correctScore = scores[range(N), y]  # (N, )
    correctScoreMat = correctScore.T * np.ones( scores.T.shape )
    correctScoreMat = correctScoreMat.T  # (N, K)로 dimension 맞춰줌

    marginMat = np.ones(scores.shape) * delta  # (N, K) delta만큼 다 더해주기 위해
    L = np.maximum(scores - correctScoreMat + marginMat, 0)
    L[range(N), y] = 0  # y_i인 부분은 0이 되어야하니까!

    data_loss = np.sum(L)/N

    reg_loss = 0.5*reg*np.sum(W*W)
    loss = data_loss + reg_loss

    if i % 10 == 0:
        print("iteration {}: loss {}".format(i, loss))
    # backward
    dscores = np.ones(L.shape)
    dscores[L == 0] = 0
    dscores /= N

    dW = np.dot(X.T, dscores)
    db = np.sum(dscores, axis=0, keepdims=True)
    dW += reg*W

    W += -step_size * dW
    b += -step_size * db

# evaluate training set accuracy
scores = np.dot(X, W) + b
predicted_class = np.argmax(scores, axis=1)  # (N, )

print('training accuracy: %.2f' % (np.mean(predicted_class == y)))

```

