---
layout: post
title:  "Neural Arithmetic Logic Units"
date:   2018-08-08
category: "machine learning"
tags: [ml, NALU, regression]
---
[논문 링크](https://arxiv.org/abs/1808.00508)

# 아이디어
* output이 numerical일 때 extrapolation도 잘되는 네트워크를 만들고 싶음
  * ![0.png](/resources/1CBA9348880853690D60A781ECF722BA.png)
  * 이게 현재의 activation function들의 한계
* extrapolation을 잘 하려면, primitive 수학 연산들을 할 수 있는 linear activation가 있어야하지 않을까?
* 해서 다음과 같은 것들을 만들었음
  * ![1.png](/resources/FE94F6F047E7DF817791A88F82ABFB7D.png)
  * ![2.png](/resources/045C921D000AE10CBC28A260089F721C.png)
  * $W$는 [-1, 0, 1] 사이의 값을 갖기를 원했음
    * 그러면 NAC($a = Wx$)는 덧셈, 뺄셈이 가능한 모듈이 됨
    * 근데 gradient도 잘 흐르고 유사하게 가기위해 저런 식으로 만들었다함.

# 실험
* in/output이 모두 numeric인 실험
  * ![3.png](/resources/42482C6AAB5429260410FE58C18EF28E.png)
  * extrapolation이 굉장히 잘된다.
  * 노란색 부위는 왜 그런지 모르겠네...
* Mnist 10개를 넣고 인식/덧셈 연산하는 테스트
  * ![4.png](/resources/F302E76B905C020BD2BAF3678157562D.png)
  * input은 numeric이 아니며, output이 numeric
  * 덧셈과 identity니까 NAC가 더 잘됨
  * NALU는 왜 addition에서 에러가 클까...
* 숫자를 영어로 읽은 것을 숫자로 translation하는 태스크
  * ![5.png](/resources/57A5AECC0BBEF6DA927DC3ED250063E7.png)
  * NALU가 더 잘된다.
  * 중간에 prediction 결과를 찍어본 것
    * ![6.png](/resources/1D5F2072E4A15512702F458241FAF140.png)
* program evaluation은 건너뛴다. 아마 위의 translation이 되는 것을 보고 확장해서 테스트해봤을 것 같은데..
* 도착 시간을 주고 그 시간에 도착하면 reward를 주는 환경
  * ![7.png](/resources/4124A3BBE07DA6A3A1A32618E1F654BF.png)
  * action space: `{UP, DOWN, LEFT, RIGHT, PASS}`
  * convnet output과 도착해야할 시간이 concat해서 LSTM에 들어감