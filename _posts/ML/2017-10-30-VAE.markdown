---
layout: post
title:  "Variational AutoEncoder(VAE)"
date:   2017-10-30 
category: "machine learning"
tags: [ml]
---

# Variational autoencoder

![IMAGE]({{ site.url }}/resources/0692C74FD7CE6FB1532DB15C7DC4F1A9.jpg){:width="130px"}

* $$p(x, z) = p(x|z)p(z)$$
  * $$x$$: data
  * $$z$$: latent variable
## Generation process

1. $$z_i \sim p(z)$$를 draw한다.
2. $$x_i \sim p(x|z)$$를 draw한다.

우리가 원하는 것은 데이터가 주어졌을 때, 이를 가장 잘 표현하는 z를 찾고싶은 것!

$$p(z|x) = \frac{p(x|z)p(z)}{p(x)}$$

* 여기서 $$p(x)=∫p(x∣z)p(z)dz$$ 인데 이를 구하기 힘듬!
* Variational inference는 posterior를 어떤 distribution family들 $$q_\lambda(z\vert x)$$로 근사시킨다. 그래놓고 $$ p(z \vert x) $$와 유사한 값을 찾아나가겠지..
  * ![3.png](/resources/58DFD32E2D1E50EAB2B4B402596FCCF4.png){:width="400px"}
* 유사한 값을 찾기위해 KL-divergence를 쓰면 다음처럼 나온다.
  * $$KL(q_\lambda(z \vert x) \vert\vert p(z\vert x)) = logp(x)-\left ( E_q[log{p(x\vert z)}] - KL(q_\lambda(z\vert x) \vert\vert p(z)) \right )$$
  * 그래서 $$KL(q_\lambda(z\vert x) \vert\vert p(z\vert x))$$를 minimize하는 것은
  * $$E_q[log{p(x\vert z)}] - KL(q_\lambda(z\vert x) \vert\vert p(z))$$ 를 maximize하는 것!
  * $$p(x\vert z)$$가 decoder, $$q_\lambda(z\vert x)$$를 encoder
> 식 유도..
![1.png]({{ site.url }}/resources/F73E1A9A598FF33975F2D141F8EBC8CF.png){:width="773px"}
![2.png]({{ site.url }}/resources/5269563422231A97269411038931B2E1.png){:width="835px"}
